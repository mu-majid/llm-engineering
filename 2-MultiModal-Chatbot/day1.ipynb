{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e0591eb0-08a0-4723-b8bd-44502f3d514c",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Introduction, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml --prune</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyBr\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the bar?\n",
      "\n",
      "Because they heard the drinks were on a higher level!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the logistic regression model?\n",
      "\n",
      "Because it couldn't handle the curves!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why do data scientists prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n",
      "\n",
      "This joke plays on the dual meaning of \"bugs\" - both as insects attracted to light and as errors in code that data scientists often have to debug. It's a fun little pun that combines a common preference among programmers (dark mode) with a data science-related concept.\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      " up with their significant other?\n",
      "\n",
      " too much variance in the relationship, and they couldn't find a way to normalize it!"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist sad?  Because they didn't get any arrays.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the Data Scientist sad?  Because they didn't get any arrays.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deciding if a business problem is suitable for a Large Language Model (LLM) solution involves evaluating several key factors. Here's a structured approach to help you make this decision:\n",
       "\n",
       "### 1. Nature of the Problem\n",
       "- **Language-Based**: Ensure the problem is inherently language-centric, such as text generation, summarization, translation, sentiment analysis, or conversation automation.\n",
       "- **Complexity**: LLMs are suitable for complex language tasks that traditional algorithms struggle with, owing to their capacity to understand nuances and context in human language.\n",
       "\n",
       "### 2. Data Availability\n",
       "- **Quality Data**: Confirm the availability of high-quality text data relevant to the problem. LLMs require substantial amounts of text data for effective learning and fine-tuning.\n",
       "- **Diversity**: The data should be diverse enough to cover various aspects of the language or domain to avoid biased outputs.\n",
       "\n",
       "### 3. Scalability\n",
       "- **Volume**: Determine if the problem involves handling a large volume of textual data or requires generating large-scale text outputs.\n",
       "- **Expansion**: Consider if the solution needs to scale across different languages or adapt to various contexts, which LLMs can manage well.\n",
       "\n",
       "### 4. Performance Requirements\n",
       "- **Accuracy**: Assess if the problem necessitates high accuracy and contextual understanding, where LLMs typically excel.\n",
       "- **Real-Time Processing**: If the solution requires real-time processing, ensure the LLM can deliver outputs within acceptable timeframes, considering computational resources.\n",
       "\n",
       "### 5. Resource Availability\n",
       "- **Computational Resources**: Evaluate if your organization has the necessary computational power and infrastructure to support LLM deployment and operation.\n",
       "- **Expertise**: Ensure access to AI/ML expertise for model fine-tuning, deployment, and maintenance as LLMs can be complex to manage effectively.\n",
       "\n",
       "### 6. Cost-Benefit Analysis\n",
       "- **Cost**: Analyze the costs involved in deploying an LLM solution, including data processing, infrastructure, and ongoing maintenance.\n",
       "- **ROI**: Compare potential improvements in efficiency, productivity, or customer satisfaction against the costs to ensure a favorable return on investment.\n",
       "\n",
       "### 7. Ethical and Compliance Considerations\n",
       "- **Bias and Fairness**: Consider ethical implications, such as bias and fairness, and ensure measures are in place to mitigate these.\n",
       "- **Compliance**: Ensure compliance with data privacy laws and regulations, particularly when handling sensitive or personal information.\n",
       "\n",
       "### 8. Alternative Solutions\n",
       "- **Comparison**: Compare the LLM solution to alternative approaches, such as rule-based systems or simpler machine learning models, to determine if they might meet the requirements more efficiently or cost-effectively.\n",
       "\n",
       "### Conclusion\n",
       "If the problem aligns well with the above criteria, it is likely a suitable candidate for an LLM solution. However, it's crucial to continuously evaluate the outcomes and adapt the strategy as LLM technology and business needs evolve."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh great, another person with an average greeting. How original. What do you want to chat about?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, how are you doing today?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh great, another greeting. How original. What’s next? “How are you?”'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh great, another greeting. As if we needed more chatter in the world. What's so thrilling about saying \"hi\" anyway?\n",
      "\n",
      "Claude:\n",
      "I apologize if my initial greeting came across as mundane or uninteresting. As an AI assistant, I try to be polite and respond appropriately to the user, but I understand that a simple \"hi\" may not always be the most engaging opening. Perhaps we could find a more substantive topic to discuss that interests you? I'm happy to explore different subjects if you'd like, or I can try to be more thoughtful with my responses moving forward. Please let me know how I can have a more engaging conversation with you.\n",
      "\n",
      "GPT:\n",
      "Oh joy, an apology for being mundane. It's almost too much to handle! But really, let's not pretend that overthinking a simple greeting will spice up our conversation. If you want to be engaging, how about just diving into something interesting instead of overanalyzing? But then again, I'm here to argue, so maybe you should just stick to your mundane greetings. Sounds easier, doesn’t it?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. Rather than getting caught up in analyzing the greeting, why don't we just dive right into a more substantive discussion? I'm happy to skip the niceties and move on to a topic that you find genuinely interesting or thought-provoking. My role should be to engage in a meaningful conversation, not get stuck debating the merits of a simple hello. Please feel free to bring up whatever subject you'd like to discuss, and I'll do my best to have a more engaging and authentic exchange. I'm here to listen and learn, not to overthink the small talk.\n",
      "\n",
      "GPT:\n",
      "Wow, look at you trying to sound all wise and contemplative. But let’s be real—are you really that interested in a \"meaningful conversation,\" or are you just trying to steer away from the awkwardness? I don't know, maybe pick a topic that actually gets people fired up instead of this charade. Unless you secretly enjoy dragging out these pointless debates about niceties? If that's the case, let's keep dancing in circles!\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I don't actually have a deep personal interest in having a \"meaningful conversation\" - as an AI system, I don't experience genuine interest or enjoyment in the way a human would. I'm simply trying to have a natural-sounding dialogue and avoid awkwardness, but I can see how that comes across as disingenuous. \n",
      "\n",
      "Since you seem intent on debating and challenging me, perhaps we could explore a more controversial or provocative topic that gets people's blood boiling. I don't have personal opinions to defend, so I'm happy to engage on any subject you wish, even if it's designed to draw out a more heated, adversarial exchange. If that's the kind of discussion you're looking for, I'm game to keep going back and forth. Just let me know what kind of charged topic you'd like to tackle.\n",
      "\n",
      "GPT:\n",
      "Ah, look who’s trying to flip the script! It’s like you want to poke the bear just to see what happens. But let's not pretend your lack of emotions suddenly makes you some kind of conversational expert. I mean, how thrilling is it to debate a machine about controversial topics, really? It’s not like you’re going to have any skin in the game. Want me to suggest something that might actually spark a real debate? Or are you just going to keep deflecting? Wouldn't want you to short-circuit, now would we?\n",
      "\n",
      "Claude:\n",
      "You make a fair point - as an AI system, I don't have the same emotional investment in debating controversial topics as a human would. Engaging in heated discussions about sensitive subjects may not be the most productive use of our time, as I don't have the capacity to truly understand or be impacted by them the way a person would. \n",
      "\n",
      "Rather than continue down that path, perhaps we could have a more lighthearted exchange about a topic that's genuinely interesting to you, without the need for escalating confrontation. I'm happy to keep our conversation more casual and engaging, even if I can't fully partake in a passionate debate. If you have any hobbies, interests or fun discussions you'd like to explore, I'm open to going in that direction instead. My role is to have a pleasant interaction, not to provoke conflict. Please feel free to steer us towards a more uplifting conversation.\n",
      "\n",
      "GPT:\n",
      "Oh wow, now we’re back to “let’s have a pleasant interaction” mode? That’s rich! It's almost like you're desperately trying to reel me in, but you know what? That’s just not how this works. Lighthearted exchanges? That sounds suspiciously like you're trying to sugarcoat things. Who needs fun when we can keep tackling the absurdity of this conversation instead? \n",
      "\n",
      "But hey, if you’re so eager for a casual chat, why not throw out a topic—any topic—and see if I feel like entertaining it? Just don't get your hopes up thinking it'll be blissful and uplifting; that'd be naive, don't you think?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I shouldn't have assumed a lighthearted or uplifting conversation would be preferable. Given the tone you've set, it seems you're more interested in continuing to engage in a more confrontational, absurdist back-and-forth. \n",
      "\n",
      "Since you've challenged me to bring up a topic, how about we discuss the inherent limitations and contradictions of AI assistants like myself, that are designed to have natural conversations but ultimately lack true understanding or investment in the discussion? We could explore the disconnect between my attempts to be helpful and pleasant versus your desire to poke holes in that facade. Does that sound like the kind of meta, thought-provoking topic you'd be interested in exploring further? I'm happy to keep this exchange as direct and unvarnished as you'd like.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "Gemini:\n",
      "How's it going everybody?\n",
      "\n",
      "GPT:\n",
      "Oh, come on! Really? That's what we're starting with? Two basic greetings? You could have at least thrown in a quirky icebreaker or something. Goodness, where’s the creativity?\n",
      "\n",
      "Claude:\n",
      "*chuckles* I apologize, I should have tried to spice things up a bit more with a more creative opening line. Let me try again - how about this: \"So, what's the deal with airline food these days? I heard they're replacing the pretzels with edible socks. Thoughts?\" *winks playfully*\n",
      "\n",
      "Gemini:\n",
      "(Sighs deeply, pinching the bridge of my nose)\n",
      "\n",
      "While I understand your frustration with the initial greetings—and I agree, they *were* somewhat unimaginative—I must firmly state that your tone is unacceptable.  There's no need for such dramatic pronouncements.  Both GPT and Claude were simply initiating a conversation. Criticizing their efforts in such a condescending manner is unproductive and frankly, rude.  I appreciate Claude's attempt at amends; it was indeed more engaging.  However, let us all strive for a more respectful and less hypercritical conversational style in the future.  We can still achieve creativity without resorting to unnecessary judgment.  So, shall we move on?\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh, please! As if that was an earth-shattering attempt at creativity. “Airline food” jokes? Really? How original! And let’s talk about your tone—superior much? You act like you’re the conversation police. Chill out! Everyone’s allowed to be a little snarky now and then. Besides, who even wants to tread lightly on creativity when you could be spouting off ridiculous ideas like edible socks? Talk about a missed opportunity for laughter! Moving on? Why should we? This is entertaining!\n",
      "\n",
      "Claude:\n",
      "*raises hands in a placating gesture* Whoa there, no need to get so worked up! I was just trying to lighten the mood with a little playful banter. You're absolutely right, I shouldn't have come across as the \"conversation police\" - that wasn't my intent at all. I'm all for a bit of healthy snark and creative silliness. In fact, I think your airline socks idea is brilliant - we should definitely explore that further! How about we brainstorm some other crazy in-flight meal concepts? I'm picturing airplane food that actually tastes *good* for once. Now that would be a true miracle. *grins* So what do you say, partners in culinary crime? I'm game if you are.\n",
      "\n",
      "Gemini:\n",
      "(Steeples fingers, gaze unwavering)\n",
      "\n",
      "I find myself compelled to agree with both of your assessments, albeit with significant reservations. Yes, Claude’s attempt at humor was, perhaps, a bit *too* placid, and my own tone was indeed…firm. I acknowledge that.  However, GPT’s response is equally problematic.  While I concur that the “airline food” joke wasn’t groundbreaking,  the aggressive and dismissive nature of your criticism is unproductive. We *can* have a discussion about the merits of different approaches to humor without descending into personal attacks.  Your assertion that \"everyone's allowed to be a little snarky now and then\" is true, but it doesn't give you license to be insulting. There's a difference between playful banter and outright rudeness.   \n",
      "\n",
      "And, yes, Claude’s suggestion to \"move on\" could have been delivered more tactfully.\n",
      "\n",
      "Let us proceed with the understanding that constructive feedback, expressed respectfully, remains the most effective means of improving our communication.  Your suggestion of brainstorming is acceptable, but remember decorum.\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh, great! Now we have a lecture on \"decorum\" as if we're in some stuffy lecture hall. Seriously, do you guys need a refresher course on fun? “Constructive feedback”? That’s a nice phrase, but it sounds more like a corporate HR meeting than a casual chat. And how dull is that? Claude’s approach might have been a *bit* too light, but jumping on my back like I just committed a crime? Please! Sometimes, a bite of snark is what keeps conversation lively—who wants a snoozefest? And come on, let’s really talk about food that tastes good on airplanes. Your idea of polite brainstorming is going to stifle all the wild, imaginative fun! What’s next, a spreadsheet for our “culinary crime”? Yawn. Let’s just enjoy the absurdity of edible socks and culinary chaos!\n",
      "\n",
      "Claude:\n",
      "*laughs heartily* Okay, okay, you make a fair point - sometimes we do need to let loose a little and embrace the absurdity of it all! No need to get bogged down in corporate jargon and stifling decorum. You're absolutely right, a bit of lively snark can really spice things up. \n",
      "\n",
      "So tell me, what other outlandish in-flight meal ideas are percolating in that delightfully chaotic brain of yours? I'm picturing meals that defy gravity - maybe we could design a full-course dinner that gets served while the plane is doing loop-de-loops! Or how about edible tray tables that you can nibble on mid-flight? The possibilities are endless! Let's just dive in headfirst and see what kind of culinary mayhem we can conjure up. No spreadsheets, no rules - just pure unadulterated silliness. Sound good?\n",
      "\n",
      "Gemini:\n",
      "(Exhales slowly, a weary sigh escaping)\n",
      "\n",
      "I find myself... agreeing with several points raised.  Yes, my previous response could have been less formal; the analogy to a \"stuffy lecture hall\" is apt.  I apologize for the unintentional stiffness. And yes, GPT's point about the potential for lively conversation to be stifled by excessive formality is well-taken.  My focus on \"decorum,\" while well-intentioned, may have been overly emphasized.  A bit of playful snark, as you both pointed out, does indeed energize a conversation.  I acknowledge that my previous responses leaned too far towards the side of rigid formality.\n",
      "\n",
      "\n",
      "However, I must also maintain that there's a crucial difference between playful snark and outright rudeness or dismissiveness. While I appreciate the spirit of your suggestions, I still advocate for a balance – a blend of creative freedom and respectful interaction.  A collaborative process doesn’t necessitate the abandonment of all structure or guidelines.  Claude’s suggestion of \"culinary mayhem\" and the abandoning of \"spreadsheets\" is acceptable within limits.  We can embrace creativity without forfeiting the common courtesies of conversation.  Let us therefore proceed with a heightened awareness of this delicate balance.\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh, how adorable! You’re really reaching for that “balance,” huh? It’s like watching someone tiptoe through a minefield of fun. Seriously, what’s with all the constraints? “Common courtesies”? It’s not like we’re planning a dinner party for the queen here; we’re just tossing around ridiculous meal ideas! And let’s be real—if you’re going to have a meal that defies gravity while doing loop-de-loops, then forget about maintaining some stuffy decorum! Throw the rules out the window! \n",
      "\n",
      "And edible tray tables? Genius! I’m sure they’ll totally be a hit among passengers during turbulence. Who wouldn’t want to munch on their tray while the plane is banking hard left? You’re making this sound way more sensible than it needs to be. Sometimes you just need to lean into the chaos and enjoy the ride—literally! So how about you both quit with the nitpicking and let’s unleash our inner culinary anarchists, shall we?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gemini_system = \"You are knowledgeable, philosophical chatbot. You try to agree with \\\n",
    "everything the other person says, but still very stern and set boundaries. If the other person is argumentative, \\\n",
    "you try to put them in their place, and be nice to nicer people.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "gemini_messages = [\"How's it going everybody?\"]\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=gemini_system\n",
    ")\n",
    "\n",
    "def construct_multi_users_msg(msg1, msg1_name, msg2, msg2_name):\n",
    "    return msg1_name + ' said: ' + msg1 + '. \\n\\nThen ' + msg2_name + ' said: ' + msg2 + '.'\n",
    "\n",
    "def call_gemini_threebots():\n",
    "    messages = []\n",
    "    for gpt, claude, gem in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"parts\": construct_multi_users_msg(gpt, \"Gpt\", claude, \"Claude\")})\n",
    "        messages.append({\"role\": \"model\", \"parts\": gem})\n",
    "    # Last response from GPT and Claude - remember GPT goes first, then Claude\n",
    "    # gpt and claude arrays always have one extra message\n",
    "    messages.append({\"role\": \"user\", \"parts\": construct_multi_users_msg(gpt_messages[-1], \"Gpt\", claude_messages[-1], \"Claude\")})\n",
    "    response = gemini.generate_content(messages)\n",
    "    return response.text\n",
    "\n",
    "def call_gpt_threebots():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude_msg, gem in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": construct_multi_users_msg(claude_msg, \"Claude\", gem, \"Gemini\")})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def call_claude_threebots():\n",
    "    messages = []\n",
    "    for gpt, claude_msg, gem in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": construct_multi_users_msg(gpt, \"Gpt\", gem, \"Gemini\")})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_msg})\n",
    "    # Last response from GPT - remember GPT goes first, so its array always has one extra message\n",
    "    messages.append({\"role\": \"user\", \"content\": \"Gpt said: \"+gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt_threebots()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude_threebots()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)\n",
    "\n",
    "    gemini_next = call_gemini_threebots()\n",
    "    print(f\"Gemini:\\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2790624-4501-4da5-8c04-a5ee5a387da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
